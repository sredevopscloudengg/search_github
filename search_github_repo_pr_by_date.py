import json
import sys
import os
import requests
from datetime import datetime

# this program will search the github repo's for pull requests using a specific search criteria defined in the app.config file and will generate a pr summary report
# app.config: this is a configuration file for this program that contains values for github token, api_endpoint, org, repo, start_date, end_date, user_email and etc.
# pr_states.txt: this is one of the input files that contains values pr states : (open, closed)
# pr_dates.txt: this is one of the input files that contains values pr date types : (created, updated)

# usage: python <path_to_program>/search_github_repo_pr_by_date.py <path_to_config>/app.config <path_to_input>/pr_states.txt <path_to_input>/pr_dates.txt
# example(same directory): python search_github_repo_pr_by_date.py app.config pr_states.txt pr_dates.txt
# example(different directory) : python /tmp/search_github_repo_pr_by_date.py /tmp/app.config /tmp/pr_states.txt /tmp/pr_dates.txt

# this function reads the command lines arguments
def process_cmd_args():
  global GHUB_APP_CONFIG
  global GHUB_PR_STATES
  global GHUB_PR_DATES

  GHUB_APP_CONFIG = sys.argv[1]
  GHUB_PR_STATES = sys.argv[2]
  GHUB_PR_DATES = sys.argv[3]

# this function reads values from the configuration file: app.config that are used throught this program
def process_app_config():
  global ghub_api_token
  global ghub_api_search_endpoint
  global ghub_org_name
  global ghub_repo_name
  global ghub_search_start_date
  global ghub_search_end_date
  global ghub_search_results_count
  global user_email
  global app_dir
  global app_default_dir

  with open(GHUB_APP_CONFIG, "r") as config_file:
    app_config_data_dict = json.load(config_file)
    
    ghub_api_token = app_config_data_dict['token']
    ghub_api_search_endpoint = app_config_data_dict['api_endpoint']
    ghub_org_name = app_config_data_dict['org']
    ghub_repo_name = app_config_data_dict['repo']
    ghub_search_start_date = app_config_data_dict['start_date']
    ghub_search_end_date = app_config_data_dict['end_date']
    ghub_search_results_count = app_config_data_dict['search_results_count']
    user_email = app_config_data_dict['email']
    app_dir = app_config_data_dict['dir']
    app_default_dir = app_config_data_dict['default_dir']

# this function creates temp directories to store output files generated by this program
def create_temp_dirs():
  global parent_dir
  global output_dir
  global current_date_ts
  parent_dir = app_dir
  now = datetime.now() # current date and time
  current_date = now.strftime("%Y%m%d")
  current_time = now.strftime("%H%M%S")
  current_date_ts = current_date + "_" +current_time
  child_dir = current_date_ts
  new_dir = "/" + parent_dir + "/" + child_dir
  os.mkdir(new_dir)
  if os.path.isdir(new_dir):
    output_dir = new_dir
  else:
    output_dir = app_default_dir
  print(output_dir)

# this function creates an output file that contains consolidated output generated by this program used as an input for the mail program
def build_consolidated_output_file():
  global search_consolidated_output
  consolidated_format = "consolidated_data.txt"
  output_file = current_date_ts + "_" + consolidated_format
  search_consolidated_output = output_dir + "/" + output_file
  print(search_consolidated_output)

# this function creates multiple temporary output files used by the search function to write output
def build_output_file(state, date_type, format_type):
  ghub_pr_state = state
  ghub_pr_date_type = date_type
  file_format = format_type
  output_file = current_date_ts + "_" + ghub_pr_state + "_" + ghub_pr_date_type + "_" + file_format
  #print("output_file")
  #print(output_file)
  return output_file

# this function builds a search string used by the GitHub Search API using the values defined in the app.config file
def build_search_string(state, date_type):
  ghub_pr_state = state
  ghub_pr_date_type = date_type
  #ghub_search_string = "repo:ghub_org_name/ghub_repo_name+is:pr+state:$ghub_pr_state+$ghub_pr_date_type:$ghub_search_start_date..$ghub_search_end_date"
  ghub_search_string =  "repo:" + ghub_org_name + "/" + ghub_repo_name + "+is:pr+state:" + ghub_pr_state + "+" + ghub_pr_date_type + ":" + ghub_search_start_date + ".." + ghub_search_end_date
  return ghub_search_string

# this function executes search using the GitHub search api
def run_search(state, date_type):
  ghub_pr_state = state
  ghub_pr_date_type = date_type
  ghub_search_string = build_search_string(ghub_pr_state, ghub_pr_date_type)
  #print("ghub_search_string")
  print(ghub_search_string)
  
  #raw csv output
  raw_format = "raw_data.csv"
  search_raw_output = output_dir + "/" + build_output_file(ghub_pr_state, ghub_pr_date_type, raw_format)
  query_url_str = "https://{}?q={}&per_page={}"
  query_url = query_url_str.format(ghub_api_search_endpoint,ghub_search_string,ghub_search_results_count)
  print(query_url)
  headers = {'Authorization': 'token {}'.format(ghub_api_token)}
  response = requests.get(query_url, headers=headers)
  json_data_dict = json.loads(response.text)
  json_data_list = json_data_dict['items']
  number = ""
  html_url = ""
  url = ""
  temp_main_list = []
  for item_dict in json_data_list:
    temp_list = []
    for key in item_dict:
      if key == 'number':
        number = item_dict[key]
        temp_list.append(number)
        temp_list.append(",")
      if key == 'html_url':
        html_url = item_dict[key]
        temp_list.append(html_url)
  	temp_list.append(",")
      if key == 'url':
        url = item_dict[key]
        temp_list.append(url)
    temp_str = ' '.join([str(elem) for elem in temp_list])
    temp_str = temp_str.replace(" ","")
    temp_main_list.append(temp_str)
    
  with open(search_raw_output, 'w') as writer:
    for item_str in temp_main_list:
      writer.write(item_str+"\n") 

  # consolidated output 
  comment_str = "PR list with search criteria: (state:{}".format(ghub_pr_state) + "," + " date_type:{}".format(ghub_pr_date_type) + ")" + "\n"
  with open(search_raw_output, 'r') as reader, open(search_consolidated_output, 'a') as writer:
    writer.write(comment_str)
    for line in reader:
      writer.write(line)
    writer.write("\n")
      
# this is a parent search function that will loop through all values in the input files: pr_states.txt, pr_dates.txt and uses the run_search helper function to execute the search
def search_pr_date_range():
  with open(GHUB_PR_STATES, 'r') as reader1:
    for line in reader1:
      pr_state = line
      pr_state = pr_state.strip()
      with open(GHUB_PR_DATES, 'r') as reader2:
        for line in reader2:
          pr_date_type = line
          pr_date_type = pr_date_type.strip()
          # execute search
          run_search(pr_state, pr_date_type)

# main function
def main():
  process_cmd_args()
  process_app_config()
  create_temp_dirs()
  build_consolidated_output_file()
  search_pr_date_range()

# calling main function
if __name__ == "__main__":
  main()

